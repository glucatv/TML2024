{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd10b230",
   "metadata": {},
   "source": [
    "# Classificazione e minimizzazione\n",
    "\n",
    "Sia $D = [ (x^{(1)}, y^{(1)}),\\ldots,(x^{(n)}, y^{(n)}) ]$ una sequenza di vettori di *features* in $R^d$ ed etichette in $\\{-1, 1\\}$.\n",
    "\n",
    "Si vuole trovare un iperpiano $d$-dimensionale $a,a_0$ che minimizzi il *training error*\n",
    "\n",
    "$$Err(a,a_0) = \\frac{1}{n}\\sum_{i=1}^{n} L( a, a_0, x^{(i)}, y^{(i)} ) $$\n",
    "\n",
    "\n",
    "dove $L$ è la *loss* function\n",
    "\n",
    "$$\n",
    "    L( a, a_0, x^{(i)}, y^{(i)} ) = \\left\\{\n",
    "    \\begin{array}{lll}\n",
    "    0& &\\mbox{if}\\ sign(a^T\\cdot x^{(i)} + a_0) \\equiv y^{(i)}\\\\\n",
    "    1& &\\mbox{otherwise}\n",
    "    \\end{array}\n",
    "    \\right.\n",
    "$$\n",
    "\n",
    "Il problema di minimizzare la funzione $Err$ con la funzione $L$ è NP-hard: non è noto alcun algoritmo efficiente ma sono noti solo algoritmi di complessià esponenziale, ovvero algoritmi il cui tempo di calcolo cresce esponenzialmente con la dimensione del problema ($n\\times d$).\n",
    "\n",
    "![logistic](logistic.png)\n",
    "\n",
    "Se la funzione loss fosse monotona e derivabile il problema diventerebbe trattabile. Con la funzione logistica otteniamo questa proprietà. Nella figura 0.5 indica la *soglia di previsione*.\n",
    "\n",
    "![classificazione](classificazione.png)\n",
    "\n",
    "Interpretiamo $g^{(i)}$ come la probabilità che $x^{(i)}$ sia classificato correttamente. Quindi, assumendo gli $x^{(i)}$ indipendenti andiamo a massimizzare il prodotto delle probabilità. Passando al logaritmo ed invertendo il segno la funzione da minimizzare diventa\n",
    "\n",
    "\n",
    "![trainerrorf](trainerrorf.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcce300",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "![gd](gd.png)\n",
    "\n",
    "Si parte da un valore qualsiasi $z_0$ e si migliora ad ogni passo seguendo la pendenza della curva. In una dimensione: se la derivata nel punto corrente è negativa cisi sposta a destra di un valore `step` altrimenti a sinistra fintanto che non si ottengono modifiche significativa nel valore della funzione tra due punti successsivi.\n",
    "\n",
    "![gdalg](gdalg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38dd85b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cee7166",
   "metadata": {},
   "source": [
    "$$x^2 + y^2 + 3x + 3$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "158d105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(v):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    v : vettore colonna \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TYPE\n",
    "        float, il valore di f(v) dove le righe di v sono i valore delle variabili\n",
    "\n",
    "    '''\n",
    "    x = v[0][0]; y = v[1][0]\n",
    "    return x**2+y**2+3*x+3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d3971e",
   "metadata": {},
   "source": [
    "![conica](conica.png)\n",
    "\n",
    "Ha il minimo in $(-1.5, 0)$\n",
    "\n",
    "Ecco la funzione che ritorna il vettore delle derivate parziali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4259fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(v):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    v : vettore colonna \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TYPE\n",
    "        un vettore colonna contenente le derivate parziali di f\n",
    "\n",
    "    '''\n",
    "    x = v[0][0]; y = v[1][0]\n",
    "    return np.array( [ [2*x + 3], [2*y] ]  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5aac15",
   "metadata": {},
   "source": [
    "L'algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "869ec867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.4984406]\n",
      " [ 0.       ]]\n"
     ]
    }
   ],
   "source": [
    "def gd(f, df, x0, step, eps, max_iter):\n",
    "    curr_x = x0\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        curr_grad =  df(curr_x)\n",
    "\n",
    "        next_x = curr_x - step * curr_grad\n",
    "        \n",
    "        if abs( f(next_x) - f(curr_x) ) < eps:\n",
    "            break\n",
    "        \n",
    "        curr_x = next_x\n",
    "    \n",
    "    return curr_x\n",
    "\n",
    "\n",
    "ans  = gd(f, df, np.array([[0., 0.]]).T, 0.01, 0.0000001, 10000)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28968cb",
   "metadata": {},
   "source": [
    "Il gradiente può essere sostituto dal vettore delle differenze finite, la componente $i$-esima è definita in questo modo dove $\\delta$ appare in posizione $i$ del vettore di zeri.\n",
    "\n",
    "$$\n",
    "\\frac{f(x + [0,...,0,\\delta, 0,...0] ) - f(x - [0,...,0,\\delta, 0,...0] ) }{2\\delta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44e179d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.51989532]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def num_grad(f, delta=0.000000000000001):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : func\n",
    "    delta : float The default is 0.0001.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a funct that is the numerical gradient of f with increment delta\n",
    "    '''\n",
    "    \n",
    "    def df(x):\n",
    "        g = np.zeros(x.shape)\n",
    "        for i in range(x.shape[0]):\n",
    "            delta_v = np.zeros(x.shape)\n",
    "            delta_v[i,0] = delta\n",
    "            g[i, 0] = ( f( x + delta_v) - f( x - delta_v) ) / (2*delta)\n",
    "        return g\n",
    "    return df\n",
    "    \n",
    "ans  = gd(f, num_grad(f), np.array([[0., 0.]]).T, 0.01, 0.0000001, 10000)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c61008",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
